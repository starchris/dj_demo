"""
ä¼ä¸šä¿¡æ¯é‡‡é›†æ¨¡å— - é€šè¿‡å…¬å¼€æ¸ é“æœç´¢ä¼ä¸šç»è¥ä¿¡æ¯å’Œæ‹›è˜ä¿¡æ¯
Company Researcher Module - Gather company info from public channels

é‡‡é›†ç»´åº¦ï¼š
  1. ä¼ä¸šåŸºæœ¬ä¿¡æ¯ä¸è´¢åŠ¡çŠ¶å†µï¼ˆæœç´¢å¼•æ“ + ä¼ä¸šä¿¡æ¯å¹³å°ï¼‰
  2. ä¸šåŠ¡å‘å±•åŠ¨æ€ï¼ˆæ–°é—»æœç´¢ï¼‰
  3. æ‹›è˜ä¿¡æ¯ä¸äººæ‰éœ€æ±‚ï¼ˆæ‹›è˜å¹³å°æœç´¢ï¼‰

æ³¨æ„ï¼šæ­¤æ¨¡å—é€šè¿‡æœç´¢å¼•æ“é—´æ¥è·å–å…¬å¼€ä¿¡æ¯ï¼Œä½œä¸º LLM åˆ†æçš„è¡¥å……è¾“å…¥ã€‚
     å½“ LLM æœ¬èº«æ”¯æŒè”ç½‘æœç´¢æ—¶ï¼ˆå¦‚ Kimi K2.5ï¼‰ï¼Œæ­¤æ¨¡å—çš„ç»“æœä½œä¸ºé¢å¤–å‚è€ƒã€‚
"""

import logging
import re
import time
from dataclasses import dataclass, field
from typing import Optional
from urllib.parse import quote

import requests
from bs4 import BeautifulSoup

from .config import DEFAULT_HEADERS, MAX_SEARCH_RESULTS, REQUEST_TIMEOUT

logger = logging.getLogger("biz_intel_agent.researcher")


@dataclass
class SearchResult:
    """æœç´¢ç»“æœæ•°æ®ç±»"""
    title: str
    url: str
    snippet: str = ""
    source: str = ""


@dataclass
class CompanyResearchData:
    """
    ä¼ä¸šè°ƒç ”æ•°æ® - åŒ…å«ä»å„æ¸ é“æ”¶é›†çš„åŸå§‹ä¿¡æ¯

    å…­ä¸ªé‡‡é›†ç»´åº¦ï¼ˆå¯¹é½ SKILL.md çš„åˆ†ææ¡†æ¶ï¼‰ï¼š
    1. ä¼ä¸šåŸºæœ¬ä¿¡æ¯
    2. èèµ„ä¿¡æ¯ï¼ˆå…³é”®ç»´åº¦ï¼šè½®æ¬¡ã€é‡‘é¢ã€èµ„é‡‘ç”¨é€” â†’ äººæ‰æ–¹å‘ï¼‰
    3. è´¢åŠ¡/ç»è¥ä¿¡æ¯
    4. ä¸šåŠ¡å‘å±•/æ–°é—»åŠ¨æ€
    5. æ‹›è˜ä¿¡æ¯ä¸äººæ‰éœ€æ±‚
    6. è¡Œä¸šç«äº‰/å¸‚åœºä¿¡æ¯
    """
    company_name: str
    # ä¼ä¸šåŸºæœ¬ä¿¡æ¯
    basic_info: list[SearchResult] = field(default_factory=list)
    # èèµ„ä¿¡æ¯ï¼ˆä¸“é¡¹é‡‡é›†ï¼šè½®æ¬¡ã€é‡‘é¢ã€æŠ•èµ„æ–¹ã€èµ„é‡‘ç”¨é€”ï¼‰
    funding_info: list[SearchResult] = field(default_factory=list)
    # è´¢åŠ¡/ç»è¥ä¿¡æ¯
    financial_info: list[SearchResult] = field(default_factory=list)
    # ä¸šåŠ¡å‘å±•/æ–°é—»åŠ¨æ€
    business_news: list[SearchResult] = field(default_factory=list)
    # æ‹›è˜ä¿¡æ¯
    recruitment_info: list[SearchResult] = field(default_factory=list)
    # è¡Œä¸šç«äº‰/å¸‚åœºä¿¡æ¯
    market_info: list[SearchResult] = field(default_factory=list)

    def to_prompt_text(self) -> str:
        """
        å°†æ‰€æœ‰é‡‡é›†åˆ°çš„ä¿¡æ¯æ ¼å¼åŒ–ä¸º LLM å¯è¯»çš„æ–‡æœ¬

        æŒ‰ç»´åº¦ç»„ç»‡ï¼Œèèµ„ä¿¡æ¯æ”¾åœ¨é å‰çš„ä½ç½®ï¼ˆæœ€å…³é”®çš„åˆ†æç»´åº¦ï¼‰
        """
        sections = []

        sections.append(f"# å…³äºã€Œ{self.company_name}ã€çš„å…¬å¼€ä¿¡æ¯é‡‡é›†ç»“æœ\n")

        if self.basic_info:
            sections.append("## ä¸€ã€ä¼ä¸šåŸºæœ¬ä¿¡æ¯")
            for i, item in enumerate(self.basic_info, 1):
                sections.append(f"{i}. ã€{item.title}ã€‘")
                if item.snippet:
                    sections.append(f"   {item.snippet}")
            sections.append("")

        # èèµ„ä¿¡æ¯æ”¾åœ¨å‰é¢ï¼ˆSKILL.md æ ‡æ³¨ä¸º Criticalï¼‰
        if self.funding_info:
            sections.append("## äºŒã€èèµ„ä¿¡æ¯ï¼ˆå…³é”®ç»´åº¦ï¼‰")
            sections.append("è¯·é‡ç‚¹åˆ†æï¼šèèµ„è½®æ¬¡ã€é‡‘é¢ã€æŠ•èµ„æ–¹ã€èµ„é‡‘ç”¨é€” â†’ å¯¹åº”çš„äººæ‰æ‹›è˜æ–¹å‘")
            for i, item in enumerate(self.funding_info, 1):
                sections.append(f"{i}. ã€{item.title}ã€‘")
                if item.snippet:
                    sections.append(f"   {item.snippet}")
            sections.append("")

        if self.financial_info:
            sections.append("## ä¸‰ã€è´¢åŠ¡ä¸ç»è¥ä¿¡æ¯")
            for i, item in enumerate(self.financial_info, 1):
                sections.append(f"{i}. ã€{item.title}ã€‘")
                if item.snippet:
                    sections.append(f"   {item.snippet}")
            sections.append("")

        if self.business_news:
            sections.append("## å››ã€ä¸šåŠ¡å‘å±•ä¸æ–°é—»åŠ¨æ€")
            for i, item in enumerate(self.business_news, 1):
                sections.append(f"{i}. ã€{item.title}ã€‘")
                if item.snippet:
                    sections.append(f"   {item.snippet}")
            sections.append("")

        if self.recruitment_info:
            sections.append("## äº”ã€æ‹›è˜ä¿¡æ¯ä¸äººæ‰éœ€æ±‚")
            sections.append("è¯·é‡ç‚¹åˆ†æï¼šå„å¹³å°èŒä½æ•°é‡ã€è–ªèµ„èŒƒå›´ã€æ¸ é“åˆ†å¸ƒã€çŒè˜å æ¯”")
            for i, item in enumerate(self.recruitment_info, 1):
                sections.append(f"{i}. ã€{item.title}ã€‘")
                if item.snippet:
                    sections.append(f"   {item.snippet}")
            sections.append("")

        if self.market_info:
            sections.append("## å…­ã€è¡Œä¸šç«äº‰ä¸å¸‚åœºä¿¡æ¯")
            for i, item in enumerate(self.market_info, 1):
                sections.append(f"{i}. ã€{item.title}ã€‘")
                if item.snippet:
                    sections.append(f"   {item.snippet}")
            sections.append("")

        return "\n".join(sections)

    @property
    def has_data(self) -> bool:
        """æ˜¯å¦é‡‡é›†åˆ°äº†æœ‰æ•ˆæ•°æ®"""
        return bool(
            self.basic_info or self.funding_info or self.financial_info or
            self.business_news or self.recruitment_info or self.market_info
        )


class CompanyResearcher:
    """
    ä¼ä¸šä¿¡æ¯è°ƒç ”å™¨

    é€šè¿‡å¤šä¸ªæœç´¢å¼•æ“å’Œå…¬å¼€æ¸ é“é‡‡é›†ä¼ä¸šä¿¡æ¯ï¼Œ
    ä¸º LLM åˆ†ææä¾›æ•°æ®æ”¯æ’‘
    """

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(DEFAULT_HEADERS)

    def research(self, company_name: str) -> CompanyResearchData:
        """
        å¯¹æŒ‡å®šå…¬å¸è¿›è¡Œå…¨æ–¹ä½ä¿¡æ¯é‡‡é›†

        Args:
            company_name: å…¬å¸åç§°

        Returns:
            CompanyResearchData: é‡‡é›†åˆ°çš„å„ç»´åº¦ä¿¡æ¯
        """
        logger.info(f"å¼€å§‹é‡‡é›†ã€Œ{company_name}ã€çš„ä¼ä¸šä¿¡æ¯...")
        data = CompanyResearchData(company_name=company_name)

        # 1. ä¼ä¸šåŸºæœ¬ä¿¡æ¯
        logger.info(f"  ğŸ“‹ é‡‡é›†ä¼ä¸šåŸºæœ¬ä¿¡æ¯...")
        data.basic_info = self._search_basic_info(company_name)
        time.sleep(1)

        # 2. èèµ„ä¿¡æ¯ï¼ˆSKILL.md æ ‡æ³¨ä¸º Criticalï¼‰
        logger.info(f"  ğŸ¦ é‡‡é›†èèµ„ä¿¡æ¯ï¼ˆå…³é”®ç»´åº¦ï¼‰...")
        data.funding_info = self._search_funding_info(company_name)
        time.sleep(1)

        # 3. è´¢åŠ¡/ç»è¥ä¿¡æ¯
        logger.info(f"  ğŸ’° é‡‡é›†è´¢åŠ¡ç»è¥ä¿¡æ¯...")
        data.financial_info = self._search_financial_info(company_name)
        time.sleep(1)

        # 4. ä¸šåŠ¡å‘å±•ä¸æ–°é—»
        logger.info(f"  ğŸ“° é‡‡é›†ä¸šåŠ¡å‘å±•åŠ¨æ€...")
        data.business_news = self._search_business_news(company_name)
        time.sleep(1)

        # 5. æ‹›è˜ä¿¡æ¯ï¼ˆå«æ¸ é“åˆ†å¸ƒã€è–ªèµ„ï¼‰
        logger.info(f"  ğŸ‘¥ é‡‡é›†æ‹›è˜ä¿¡æ¯...")
        data.recruitment_info = self._search_recruitment_info(company_name)
        time.sleep(1)

        # 6. å¸‚åœºç«äº‰ä¿¡æ¯
        logger.info(f"  ğŸ“Š é‡‡é›†å¸‚åœºç«äº‰ä¿¡æ¯...")
        data.market_info = self._search_market_info(company_name)

        total = (
            len(data.basic_info) + len(data.funding_info) +
            len(data.financial_info) + len(data.business_news) +
            len(data.recruitment_info) + len(data.market_info)
        )
        logger.info(f"ã€Œ{company_name}ã€ä¿¡æ¯é‡‡é›†å®Œæˆï¼Œå…±è·å– {total} æ¡ç»“æœ")

        return data

    # ============================================================
    # å„ç»´åº¦æœç´¢
    # ============================================================

    def _search_basic_info(self, company_name: str) -> list[SearchResult]:
        """æœç´¢ä¼ä¸šåŸºæœ¬ä¿¡æ¯ï¼ˆè§„æ¨¡ã€ä¸»è¥ä¸šåŠ¡ç­‰ï¼‰"""
        queries = [
            f"{company_name} å…¬å¸ç®€ä»‹ è§„æ¨¡ ä¸»è¥ä¸šåŠ¡",
            f"{company_name} ä¼ä¸šä¿¡æ¯ æˆç«‹æ—¶é—´ å‘˜å·¥äººæ•°",
        ]
        results = []
        for query in queries:
            results.extend(self._search_baidu(query))
            if len(results) >= MAX_SEARCH_RESULTS:
                break
            time.sleep(0.5)
        return results[:MAX_SEARCH_RESULTS]

    def _search_funding_info(self, company_name: str) -> list[SearchResult]:
        """
        æœç´¢èèµ„ä¿¡æ¯ï¼ˆSKILL.md æ ‡æ³¨ä¸º Critical ç»´åº¦ï¼‰

        é‡ç‚¹é‡‡é›†ï¼šèèµ„è½®æ¬¡ã€é‡‘é¢ã€æ—¶é—´ã€æŠ•èµ„æ–¹ã€èµ„é‡‘ç”¨é€”
        """
        queries = [
            f"{company_name} èèµ„ è½®æ¬¡ é‡‘é¢ æŠ•èµ„æ–¹ 2024 2025",
            f"{company_name} èèµ„ èµ„é‡‘ç”¨é€” æ‰©å¼  ç ”å‘",
            f"{company_name} è·æŠ• ä¼°å€¼ èèµ„å†å²",
        ]
        results = []
        for query in queries:
            results.extend(self._search_baidu(query))
            if len(results) >= MAX_SEARCH_RESULTS:
                break
            time.sleep(0.5)
        return results[:MAX_SEARCH_RESULTS]

    def _search_financial_info(self, company_name: str) -> list[SearchResult]:
        """æœç´¢è´¢åŠ¡å’Œç»è¥ä¿¡æ¯"""
        queries = [
            f"{company_name} è¥æ”¶ åˆ©æ¶¦ è´¢æŠ¥ 2024 2025",
            f"{company_name} ä¸šç»© å¢é•¿ å¸‚å€¼",
        ]
        results = []
        for query in queries:
            results.extend(self._search_baidu(query))
            if len(results) >= MAX_SEARCH_RESULTS:
                break
            time.sleep(0.5)
        return results[:MAX_SEARCH_RESULTS]

    def _search_business_news(self, company_name: str) -> list[SearchResult]:
        """æœç´¢ä¸šåŠ¡å‘å±•å’Œæ–°é—»åŠ¨æ€"""
        queries = [
            f"{company_name} æœ€æ–°åŠ¨æ€ ä¸šåŠ¡å‘å±• æˆ˜ç•¥",
            f"{company_name} æ–°äº§å“ ä¸šåŠ¡æ‰©å¼  åˆä½œ",
        ]
        results = []
        for query in queries:
            results.extend(self._search_baidu_news(query))
            if len(results) >= MAX_SEARCH_RESULTS:
                break
            time.sleep(0.5)
        return results[:MAX_SEARCH_RESULTS]

    def _search_recruitment_info(self, company_name: str) -> list[SearchResult]:
        """
        æœç´¢æ‹›è˜ä¿¡æ¯å’Œäººæ‰éœ€æ±‚

        é‡ç‚¹é‡‡é›†ï¼šå„å¹³å°èŒä½æ•°é‡ã€è–ªèµ„èŒƒå›´ã€æ¸ é“åˆ†å¸ƒã€çŒè˜å æ¯”
        """
        queries = [
            f"{company_name} æ‹›è˜ èŒä½ è–ªèµ„ 2025",
            f"{company_name} BOSSç›´è˜ çŒè˜ æ‹›è˜ åœ¨æ‹›èŒä½æ•°",
            f"{company_name} ç¤¾æ‹› äººæ‰éœ€æ±‚ é«˜è–ª æŠ€æœ¯",
        ]
        results = []
        for query in queries:
            results.extend(self._search_baidu(query))
            if len(results) >= MAX_SEARCH_RESULTS:
                break
            time.sleep(0.5)
        return results[:MAX_SEARCH_RESULTS]

    def _search_market_info(self, company_name: str) -> list[SearchResult]:
        """æœç´¢å¸‚åœºç«äº‰ä¿¡æ¯"""
        queries = [
            f"{company_name} è¡Œä¸šåœ°ä½ ç«äº‰ å¸‚åœºä»½é¢",
        ]
        results = []
        for query in queries:
            results.extend(self._search_baidu(query))
            if len(results) >= MAX_SEARCH_RESULTS:
                break
            time.sleep(0.5)
        return results[:MAX_SEARCH_RESULTS]

    # ============================================================
    # æœç´¢å¼•æ“æ¥å£
    # ============================================================

    def _search_baidu(self, query: str) -> list[SearchResult]:
        """
        ç™¾åº¦ç½‘é¡µæœç´¢

        Args:
            query: æœç´¢å…³é”®è¯

        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        results = []
        url = f"https://www.baidu.com/s?wd={quote(query)}&rn=10"

        try:
            resp = self.session.get(url, timeout=REQUEST_TIMEOUT)
            resp.encoding = "utf-8"

            if resp.status_code != 200:
                logger.warning(f"ç™¾åº¦æœç´¢è¯·æ±‚å¤±è´¥: HTTP {resp.status_code}")
                return results

            soup = BeautifulSoup(resp.text, "html.parser")

            # è§£ææœç´¢ç»“æœ
            items = soup.select("div.result") or soup.select("div[class*='result']")

            for item in items[:MAX_SEARCH_RESULTS]:
                try:
                    title_tag = item.select_one("h3 a") or item.select_one("a")
                    if not title_tag:
                        continue

                    title = title_tag.get_text(strip=True)
                    href = title_tag.get("href", "")

                    # æ¸…ç†æ ‡é¢˜
                    title = re.sub(r'<[^>]+>', '', title).strip()
                    if not title or len(title) < 4:
                        continue

                    # æå–æ‘˜è¦
                    snippet_tag = (
                        item.select_one("div.c-summary") or
                        item.select_one("div.c-abstract") or
                        item.select_one("span.content-right_8Zs40") or
                        item.select_one("p")
                    )
                    snippet = snippet_tag.get_text(strip=True) if snippet_tag else ""
                    snippet = re.sub(r'<[^>]+>', '', snippet)[:300]

                    results.append(SearchResult(
                        title=title,
                        url=href,
                        snippet=snippet,
                        source="ç™¾åº¦æœç´¢",
                    ))
                except Exception as e:
                    logger.debug(f"è§£æç™¾åº¦æœç´¢æ¡ç›®å¤±è´¥: {e}")
                    continue

        except requests.RequestException as e:
            logger.error(f"ç™¾åº¦æœç´¢è¯·æ±‚å¼‚å¸¸: {e}")

        return results

    def _search_baidu_news(self, query: str) -> list[SearchResult]:
        """
        ç™¾åº¦æ–°é—»æœç´¢

        Args:
            query: æœç´¢å…³é”®è¯

        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        results = []
        url = f"https://news.baidu.com/ns?word={quote(query)}&tn=news&from=news&cl=2&rn=10&ct=1"

        try:
            resp = self.session.get(url, timeout=REQUEST_TIMEOUT)
            resp.encoding = "utf-8"

            if resp.status_code != 200:
                logger.warning(f"ç™¾åº¦æ–°é—»è¯·æ±‚å¤±è´¥: HTTP {resp.status_code}")
                return results

            soup = BeautifulSoup(resp.text, "html.parser")
            items = soup.select("div.result") or soup.select("div[class*='result']")

            for item in items[:MAX_SEARCH_RESULTS]:
                try:
                    title_tag = item.select_one("h3 a") or item.select_one("a")
                    if not title_tag:
                        continue

                    title = title_tag.get_text(strip=True)
                    href = title_tag.get("href", "")

                    title = re.sub(r'<[^>]+>', '', title).strip()
                    if not title or len(title) < 4:
                        continue

                    snippet_tag = (
                        item.select_one("div.c-summary") or
                        item.select_one("div.c-abstract") or
                        item.select_one("p")
                    )
                    snippet = snippet_tag.get_text(strip=True) if snippet_tag else ""
                    snippet = re.sub(r'<[^>]+>', '', snippet)[:300]

                    source_tag = item.select_one("span.c-color-gray") or item.select_one("p.c-author")
                    source = source_tag.get_text(strip=True) if source_tag else "ç™¾åº¦æ–°é—»"

                    results.append(SearchResult(
                        title=title,
                        url=href,
                        snippet=snippet,
                        source=source,
                    ))
                except Exception as e:
                    logger.debug(f"è§£æç™¾åº¦æ–°é—»æ¡ç›®å¤±è´¥: {e}")
                    continue

        except requests.RequestException as e:
            logger.error(f"ç™¾åº¦æ–°é—»è¯·æ±‚å¼‚å¸¸: {e}")

        return results

    def _search_bing(self, query: str) -> list[SearchResult]:
        """
        Bing ç½‘é¡µæœç´¢ï¼ˆç™¾åº¦æœç´¢çš„å¤‡ç”¨æ–¹æ¡ˆï¼‰

        Args:
            query: æœç´¢å…³é”®è¯

        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        results = []
        url = f"https://cn.bing.com/search?q={quote(query)}&cc=cn"

        try:
            resp = self.session.get(url, timeout=REQUEST_TIMEOUT)
            resp.encoding = "utf-8"

            if resp.status_code != 200:
                logger.warning(f"Bingæœç´¢è¯·æ±‚å¤±è´¥: HTTP {resp.status_code}")
                return results

            soup = BeautifulSoup(resp.text, "html.parser")
            items = soup.select("li.b_algo")

            for item in items[:MAX_SEARCH_RESULTS]:
                try:
                    title_tag = item.select_one("h2 a")
                    if not title_tag:
                        continue

                    title = title_tag.get_text(strip=True)
                    href = title_tag.get("href", "")

                    if not title or not href:
                        continue

                    snippet_tag = item.select_one("p") or item.select_one("div.b_caption p")
                    snippet = snippet_tag.get_text(strip=True) if snippet_tag else ""

                    results.append(SearchResult(
                        title=title,
                        url=href,
                        snippet=snippet[:300],
                        source="Bingæœç´¢",
                    ))
                except Exception as e:
                    logger.debug(f"è§£æBingæœç´¢æ¡ç›®å¤±è´¥: {e}")
                    continue

        except requests.RequestException as e:
            logger.error(f"Bingæœç´¢è¯·æ±‚å¼‚å¸¸: {e}")

        return results
