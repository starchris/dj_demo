"""
ä¸»ç¨‹åº - çƒ­ç‚¹æ–°é—»æ•æ‰å™¨
Main Entry - Hot News Catcher for 15th Five-Year Plan Industries

æµç¨‹ï¼š
  1. æŠ“å–æ–°é—»  ->  2. ç”Ÿæˆè¡Œä¸šåŠ¨æ€æ€»ç»“  ->  3. å‘é€åˆ°é£ä¹¦

ä½¿ç”¨æ–¹å¼ï¼š
    python -m news_catcher --run-once        # ç«‹å³è¿è¡Œä¸€æ¬¡
    python -m news_catcher --schedule         # å¯åŠ¨å®šæ—¶ä»»åŠ¡
    python -m news_catcher --test             # æµ‹è¯•æ¨¡å¼ï¼ˆä¸å‘é€é£ä¹¦ï¼‰
    python -m news_catcher --test-feishu      # æµ‹è¯•é£ä¹¦è¿æ¥
"""

import argparse
import json
import logging
import os
import sys
from datetime import datetime

import schedule
import time

from .config import (
    LOG_DIR,
    LOG_LEVEL,
    LLM_API_KEY,
    SCHEDULE_HOUR,
    SCHEDULE_MINUTE,
    TIMEZONE,
    INDUSTRIES,
)
from .feishu_notifier import FeishuNotifier, send_to_feishu
from .funding_fetcher import FundingEvent, fetch_funding_events
from .news_fetcher import NewsFetcher, fetch_news
from .summarizer import generate_summaries


# ============================================================
# æ—¥å¿—é…ç½®
# ============================================================
def setup_logging():
    """é…ç½®æ—¥å¿—"""
    os.makedirs(LOG_DIR, exist_ok=True)

    log_format = "%(asctime)s [%(levelname)s] %(name)s: %(message)s"
    date_format = "%Y-%m-%d %H:%M:%S"

    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(logging.Formatter(log_format, date_format))

    log_file = os.path.join(LOG_DIR, f"news_catcher_{datetime.now().strftime('%Y%m%d')}.log")
    file_handler = logging.FileHandler(log_file, encoding="utf-8")
    file_handler.setFormatter(logging.Formatter(log_format, date_format))

    root_logger = logging.getLogger("news_catcher")
    root_logger.setLevel(getattr(logging, LOG_LEVEL.upper(), logging.INFO))
    root_logger.addHandler(console_handler)
    root_logger.addHandler(file_handler)

    return root_logger


logger = None


# ============================================================
# æ ¸å¿ƒä»»åŠ¡
# ============================================================
def run_news_job(test_mode: bool = False) -> bool:
    """
    æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„æ–°é—»æŠ“å– -> æ€»ç»“ -> å‘é€æµç¨‹
    """
    global logger
    if logger is None:
        logger = setup_logging()

    logger.info("=" * 60)
    logger.info("ğŸ”¥ çƒ­ç‚¹æ–°é—»æ•æ‰å™¨ - å¼€å§‹æ‰§è¡Œ")
    logger.info(f"  æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"  æ¨¡å¼: {'æµ‹è¯•æ¨¡å¼' if test_mode else 'æ­£å¼æ¨¡å¼'}")
    logger.info(f"  LLM æ€»ç»“: {'å·²é…ç½®' if LLM_API_KEY else 'æœªé…ç½®ï¼ˆå°†ä½¿ç”¨æ ‡é¢˜æ‘˜è¦æ¨¡å¼ï¼‰'}")
    logger.info("=" * 60)

    try:
        # â”€â”€ Step 1: æŠ“å–æ–°é—» â”€â”€
        logger.info("\nğŸ“¡ Step 1/4: æŠ“å–è¡Œä¸šæ–°é—»...")
        news_by_industry = fetch_news()

        if not news_by_industry:
            logger.warning("æœªè·å–åˆ°ä»»ä½•æ–°é—»ï¼Œä»»åŠ¡ç»“æŸ")
            return False

        total_count = sum(len(items) for items in news_by_industry.values())
        logger.info(f"\nğŸ“Š æŠ“å–ç»“æœç»Ÿè®¡:")
        for industry, items in news_by_industry.items():
            emoji = INDUSTRIES.get(industry, {}).get("emoji", "ğŸ“°")
            logger.info(f"  {emoji} {industry}: {len(items)} æ¡")
        logger.info(f"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        logger.info(f"  ğŸ“° æ€»è®¡: {total_count} æ¡")

        # â”€â”€ Step 2: æŠ“å–æŠ•èèµ„/IPO äº‹ä»¶ â”€â”€
        logger.info("\nğŸ’° Step 2/4: æŠ“å–æŠ•èèµ„/IPO äº‹ä»¶...")
        funding_by_industry: dict[str, list[FundingEvent]] = {}
        try:
            funding_by_industry = fetch_funding_events()
            funding_total = sum(len(v) for v in funding_by_industry.values())
            logger.info(f"  ğŸ’° æŠ•èèµ„äº‹ä»¶: {funding_total} æ¡ï¼ˆè¦†ç›– {len(funding_by_industry)} ä¸ªè¡Œä¸šï¼‰")
            for industry, events in funding_by_industry.items():
                emoji = INDUSTRIES.get(industry, {}).get("emoji", "ğŸ“°")
                for evt in events:
                    logger.info(f"    {emoji} {evt.highlight_text()}")
        except Exception as e:
            logger.error(f"æŠ•èèµ„æŠ“å–å¤±è´¥ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰: {e}")

        # â”€â”€ Step 3: ç”Ÿæˆè¡Œä¸šåŠ¨æ€æ€»ç»“ â”€â”€
        logger.info("\nğŸ§  Step 3/4: ç”Ÿæˆè¡Œä¸šåŠ¨æ€æ€»ç»“...")
        summaries = generate_summaries(news_by_industry, funding_by_industry)

        logger.info(f"\nğŸ“ æ€»ç»“ç”Ÿæˆå®Œæˆï¼Œå…± {len(summaries)} ä¸ªè¡Œä¸šï¼š")
        for industry, summary in summaries.items():
            emoji = INDUSTRIES.get(industry, {}).get("emoji", "ğŸ“°")
            # æ˜¾ç¤ºæ€»ç»“çš„å‰ä¸¤è¡Œ
            preview_lines = summary.strip().split("\n")[:2]
            preview = " / ".join(line.strip() for line in preview_lines)
            if len(summary.strip().split("\n")) > 2:
                preview += " ..."
            logger.info(f"  {emoji} {industry}: {preview}")

        # ä¿å­˜åˆ°æœ¬åœ°ï¼ˆå¤‡ä»½ï¼Œå«æ€»ç»“ï¼‰
        save_news_to_file(news_by_industry, summaries, funding_by_industry)

        # â”€â”€ Step 4: å‘é€åˆ°é£ä¹¦ â”€â”€
        if test_mode:
            logger.info("\nğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šè·³è¿‡é£ä¹¦å‘é€")
            logger.info("\n" + "â”€" * 50)
            logger.info("ğŸ“‹ ä»¥ä¸‹ä¸ºå„è¡Œä¸šå®Œæ•´æ€»ç»“ï¼š")
            logger.info("â”€" * 50)
            for industry, summary in summaries.items():
                emoji = INDUSTRIES.get(industry, {}).get("emoji", "ğŸ“°")
                logger.info(f"\n{emoji} ã€{industry}ã€‘")
                # æ˜¾ç¤ºæŠ•èèµ„é«˜äº®
                if industry in funding_by_industry:
                    for evt in funding_by_industry[industry]:
                        logger.info(f"  ğŸ”¥ {evt.highlight_text()}")
                for line in summary.strip().split("\n"):
                    logger.info(f"  {line}")
                logger.info(f"  ï¼ˆ{len(news_by_industry.get(industry, []))} æ¡ç›¸å…³æ–°é—»ï¼‰")
            logger.info("â”€" * 50)
            return True
        else:
            logger.info("\nğŸ“¤ Step 4/4: å‘é€åˆ°é£ä¹¦...")
            success = send_to_feishu(
                news_by_industry,
                summaries=summaries,
                funding_by_industry=funding_by_industry,
            )

            if success:
                logger.info("âœ… è¡Œä¸šåŠ¨æ€é€Ÿè§ˆå·²æˆåŠŸå‘é€åˆ°é£ä¹¦ï¼")
            else:
                logger.error("âŒ é£ä¹¦å‘é€å¤±è´¥")
            return success

    except Exception as e:
        logger.error(f"âŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        return False
    finally:
        logger.info("=" * 60)
        logger.info("ä»»åŠ¡æ‰§è¡Œå®Œæ¯•\n")


def save_news_to_file(
    news_by_industry: dict,
    summaries: dict[str, str] = None,
    funding_by_industry: dict = None,
) -> None:
    """ä¿å­˜æ–°é—»ã€æ€»ç»“å’ŒæŠ•èèµ„äº‹ä»¶åˆ°æœ¬åœ° JSONï¼ˆå¤‡ä»½ï¼‰"""
    global logger
    if logger is None:
        logger = setup_logging()

    try:
        data_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "data")
        os.makedirs(data_dir, exist_ok=True)

        filename = f"news_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        filepath = os.path.join(data_dir, filename)

        serializable = {}
        for industry, items in news_by_industry.items():
            industry_data = {
                "summary": summaries.get(industry, "") if summaries else "",
                "news": [item.to_dict() for item in items],
            }
            # é™„åŠ æŠ•èèµ„äº‹ä»¶
            if funding_by_industry and industry in funding_by_industry:
                industry_data["funding_events"] = [
                    evt.to_dict() for evt in funding_by_industry[industry]
                ]
            serializable[industry] = industry_data

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(serializable, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ“ æ–°é—»å·²ä¿å­˜åˆ°: {filepath}")
    except Exception as e:
        logger.error(f"ä¿å­˜æ–°é—»æ–‡ä»¶å¤±è´¥: {e}")


# ============================================================
# å®šæ—¶ä»»åŠ¡
# ============================================================
def start_scheduler():
    """å¯åŠ¨å®šæ—¶ä»»åŠ¡"""
    global logger
    if logger is None:
        logger = setup_logging()

    schedule_time = f"{SCHEDULE_HOUR:02d}:{SCHEDULE_MINUTE:02d}"
    logger.info(f"â° å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨ï¼Œæ¯æ—¥ {schedule_time} æ‰§è¡Œ")
    logger.info(f"   æ—¶åŒº: {TIMEZONE}")
    logger.info(f"   æŒ‰ Ctrl+C åœæ­¢\n")

    schedule.every().day.at(schedule_time).do(run_news_job)

    try:
        while True:
            schedule.run_pending()
            time.sleep(60)
    except KeyboardInterrupt:
        logger.info("\nâ¹ å®šæ—¶ä»»åŠ¡å·²åœæ­¢")


def test_feishu_connection():
    """æµ‹è¯•é£ä¹¦ Webhook è¿æ¥"""
    global logger
    if logger is None:
        logger = setup_logging()

    logger.info("ğŸ§ª æµ‹è¯•é£ä¹¦ Webhook è¿æ¥...")
    try:
        notifier = FeishuNotifier()
        success = notifier.send_text(
            f"ğŸ”” çƒ­ç‚¹æ–°é—»æ•æ‰å™¨æµ‹è¯•æ¶ˆæ¯\n"
            f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
            f"çŠ¶æ€: è¿æ¥æ­£å¸¸ âœ…\n"
            f"LLM æ€»ç»“: {'å·²é…ç½®' if LLM_API_KEY else 'æœªé…ç½®'}\n"
            f"è¦†ç›–è¡Œä¸š: {', '.join(INDUSTRIES.keys())}"
        )
        if success:
            logger.info("âœ… é£ä¹¦è¿æ¥æµ‹è¯•æˆåŠŸï¼")
        else:
            logger.error("âŒ é£ä¹¦è¿æ¥æµ‹è¯•å¤±è´¥")
        return success
    except Exception as e:
        logger.error(f"âŒ é£ä¹¦è¿æ¥æµ‹è¯•å¼‚å¸¸: {e}")
        return False


# ============================================================
# å‘½ä»¤è¡Œå…¥å£
# ============================================================
def main():
    parser = argparse.ArgumentParser(
        description="ğŸ”¥ çƒ­ç‚¹æ–°é—»æ•æ‰å™¨ - åäº”äº”è§„åˆ’é‡ç‚¹è¡Œä¸šåŠ¨æ€",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python -m news_catcher --run-once        # ç«‹å³è¿è¡Œä¸€æ¬¡
  python -m news_catcher --schedule         # å¯åŠ¨å®šæ—¶ä»»åŠ¡
  python -m news_catcher --test             # æµ‹è¯•æ¨¡å¼ï¼ˆä¸å‘é€é£ä¹¦ï¼‰
  python -m news_catcher --test-feishu      # æµ‹è¯•é£ä¹¦è¿æ¥

ç¯å¢ƒå˜é‡:
  FEISHU_WEBHOOK_URL     é£ä¹¦è‡ªå®šä¹‰æœºå™¨äºº Webhook URL
  FEISHU_WEBHOOK_SECRET  é£ä¹¦ç­¾åå¯†é’¥ï¼ˆå¯é€‰ï¼‰
  LLM_API_KEY            LLM API å¯†é’¥ï¼ˆDeepSeek/Moonshot/OpenAIï¼‰
  LLM_BASE_URL           LLM API åœ°å€ï¼ˆé»˜è®¤ https://api.deepseek.comï¼‰
  LLM_MODEL              LLM æ¨¡å‹åï¼ˆé»˜è®¤ deepseek-chatï¼‰
  LOG_LEVEL              æ—¥å¿—çº§åˆ«ï¼ˆé»˜è®¤ INFOï¼‰
        """,
    )

    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("--run-once", action="store_true", help="ç«‹å³æ‰§è¡Œä¸€æ¬¡")
    group.add_argument("--schedule", action="store_true", help="å¯åŠ¨æ¯æ—¥å®šæ—¶ä»»åŠ¡")
    group.add_argument("--test", action="store_true", help="æµ‹è¯•æ¨¡å¼ï¼ˆä¸å‘é€é£ä¹¦ï¼‰")
    group.add_argument("--test-feishu", action="store_true", help="æµ‹è¯•é£ä¹¦è¿æ¥")

    parser.add_argument("--webhook-url", type=str, help="æŒ‡å®šé£ä¹¦ Webhook URL")
    parser.add_argument("--llm-key", type=str, help="æŒ‡å®š LLM API Key")

    args = parser.parse_args()

    if args.webhook_url:
        os.environ["FEISHU_WEBHOOK_URL"] = args.webhook_url
    if args.llm_key:
        os.environ["LLM_API_KEY"] = args.llm_key

    if args.run_once:
        success = run_news_job(test_mode=False)
        sys.exit(0 if success else 1)
    elif args.schedule:
        start_scheduler()
    elif args.test:
        success = run_news_job(test_mode=True)
        sys.exit(0 if success else 1)
    elif args.test_feishu:
        success = test_feishu_connection()
        sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
