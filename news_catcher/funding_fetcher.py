"""
æŠ•èèµ„äº‹ä»¶æŠ“å–æ¨¡å— - ä»æŠ•èµ„ç•Œ(pedaily.cn)è·å–æœ€æ–°æŠ•èèµ„å’ŒIPOåŠ¨æ€
Funding & IPO Fetcher - Scrape latest funding rounds and IPO events from pedaily.cn

æ•°æ®æºï¼š
1. èèµ„å¿«è®¯  https://www.pedaily.cn/first/t76/
2. IPO å‰çº¿  https://www.pedaily.cn/exit/
"""

import logging
import re
import time
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from typing import Optional
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup

from .config import DEFAULT_HEADERS, INDUSTRIES, REQUEST_TIMEOUT

logger = logging.getLogger(__name__)

# æŠ•èèµ„äº‹ä»¶çš„æ—¶æ•ˆçª—å£ï¼ˆå¤©ï¼‰
FUNDING_MAX_AGE_DAYS = 7


@dataclass
class FundingEvent:
    """æŠ•èèµ„/IPO äº‹ä»¶æ•°æ®ç±»"""
    company: str        # å…¬å¸åç§°
    title: str          # äº‹ä»¶æ ‡é¢˜
    url: str            # è¯¦æƒ…é“¾æ¥
    event_type: str     # äº‹ä»¶ç±»å‹: "èèµ„" | "IPO"
    round: str = ""     # èèµ„è½®æ¬¡ï¼ˆå¦‚ Aè½®ã€Pre-Aã€D++è½®ï¼‰æˆ– "IPO"
    amount: str = ""    # é‡‘é¢ï¼ˆå¦‚ "è¶…2äº¿å…ƒ"ã€"50.37äº¿å…ƒ"ï¼‰
    industry: str = ""  # åŒ¹é…çš„è¡Œä¸š
    publish_time: str = ""
    summary: str = ""

    def to_dict(self) -> dict:
        return {
            "company": self.company,
            "title": self.title,
            "url": self.url,
            "event_type": self.event_type,
            "round": self.round,
            "amount": self.amount,
            "industry": self.industry,
            "publish_time": self.publish_time,
            "summary": self.summary,
        }

    def highlight_text(self) -> str:
        """ç”Ÿæˆé«˜äº®å±•ç¤ºæ–‡æœ¬"""
        parts = [f"ğŸ’° {self.company}"]
        if self.event_type == "IPO":
            parts.append("IPO")
            if self.amount:
                parts.append(f"ï¼ˆ{self.amount}ï¼‰")
        else:
            if self.round:
                parts.append(f"å®Œæˆ{self.round}")
            if self.amount:
                parts.append(f"ï¼ˆ{self.amount}ï¼‰")
        return "".join(parts)


class FundingFetcher:
    """æŠ•èèµ„äº‹ä»¶æŠ“å–å™¨ - ä»æŠ•èµ„ç•Œ pedaily.cn æŠ“å–"""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(DEFAULT_HEADERS)

    def fetch_all(self) -> list[FundingEvent]:
        """
        æŠ“å–æ‰€æœ‰æœ€æ–°æŠ•èèµ„å’ŒIPOäº‹ä»¶ï¼Œå¹¶åŒ¹é…è¡Œä¸š
        Returns: [FundingEvent, ...]
        """
        events: list[FundingEvent] = []

        # 1. æŠ“å–èèµ„å¿«è®¯
        try:
            funding_events = self._fetch_funding_list()
            events.extend(funding_events)
            logger.info(f"ğŸ’° èèµ„å¿«è®¯è·å–åˆ° {len(funding_events)} æ¡")
        except Exception as e:
            logger.error(f"èèµ„å¿«è®¯æŠ“å–å¤±è´¥: {e}")

        time.sleep(1)

        # 2. æŠ“å–IPOå‰çº¿
        try:
            ipo_events = self._fetch_ipo_list()
            events.extend(ipo_events)
            logger.info(f"ğŸ”” IPOå‰çº¿è·å–åˆ° {len(ipo_events)} æ¡")
        except Exception as e:
            logger.error(f"IPOå‰çº¿æŠ“å–å¤±è´¥: {e}")

        # 3. ä¸ºæ¯ä¸ªäº‹ä»¶åŒ¹é…è¡Œä¸š
        matched = []
        for event in events:
            industry = self._match_industry(event.title + event.summary + event.company)
            if industry:
                event.industry = industry
                matched.append(event)

        logger.info(f"ğŸ“Š æŠ•èèµ„äº‹ä»¶åŒ¹é…åˆ°è¡Œä¸š: {len(matched)}/{len(events)} æ¡")
        return matched

    def _fetch_funding_list(self) -> list[FundingEvent]:
        """ä»èèµ„å¿«è®¯é¡µæŠ“å–èèµ„äº‹ä»¶"""
        events = []
        url = "https://www.pedaily.cn/first/t76/"

        try:
            resp = self.session.get(url, timeout=REQUEST_TIMEOUT)
            resp.encoding = "utf-8"

            if resp.status_code != 200:
                logger.warning(f"èèµ„å¿«è®¯è¯·æ±‚å¤±è´¥: HTTP {resp.status_code}")
                return events

            soup = BeautifulSoup(resp.text, "html.parser")

            # éå†æ‰€æœ‰ li ä¸­çš„é“¾æ¥
            for li in soup.select("li"):
                a = li.select_one("a")
                if not a:
                    continue

                title = a.get_text(strip=True)
                href = a.get("href", "")

                # è¿‡æ»¤éèèµ„æ¡ç›®
                if not href or len(title) < 10:
                    continue
                if "news.pedaily.cn" not in href:
                    continue
                if not any(kw in title for kw in ["èèµ„", "è½®", "æŠ•èµ„"]):
                    continue

                # æå–æ—¶é—´
                spans = li.select("span")
                publish_time = ""
                for span in spans:
                    text = span.get_text(strip=True)
                    if re.match(r"\d{4}-\d{2}-\d{2}", text):
                        publish_time = text
                        break

                # è¿‡æ»¤è¿‡æœŸäº‹ä»¶
                if publish_time and self._is_too_old(publish_time):
                    continue

                # è§£æèèµ„ä¿¡æ¯
                company, round_info, amount = self._parse_funding_title(title)

                event = FundingEvent(
                    company=company,
                    title=title,
                    url=href,
                    event_type="èèµ„",
                    round=round_info,
                    amount=amount,
                    publish_time=publish_time,
                )
                events.append(event)

        except requests.RequestException as e:
            logger.error(f"èèµ„å¿«è®¯è¯·æ±‚å¼‚å¸¸: {e}")

        return events

    def _fetch_ipo_list(self) -> list[FundingEvent]:
        """ä»IPOå‰çº¿é¡µæŠ“å–IPOäº‹ä»¶"""
        events = []
        url = "https://www.pedaily.cn/exit/"

        try:
            resp = self.session.get(url, timeout=REQUEST_TIMEOUT)
            resp.encoding = "utf-8"

            if resp.status_code != 200:
                logger.warning(f"IPOå‰çº¿è¯·æ±‚å¤±è´¥: HTTP {resp.status_code}")
                return events

            soup = BeautifulSoup(resp.text, "html.parser")

            # IPOé¡µé¢çš„é“¾æ¥åœ¨ a æ ‡ç­¾ä¸­
            for a in soup.select("a"):
                title = a.get_text(strip=True)
                href = a.get("href", "")

                if not href or len(title) < 15:
                    continue
                if "news.pedaily.cn" not in href:
                    continue
                if not any(kw in title for kw in ["IPO", "ä¸Šå¸‚", "æ•²é’Ÿ", "å¸‚å€¼"]):
                    continue

                # ä» URL ä¸­æå–æ—¥æœŸ (å¦‚ /202602/ -> 2026å¹´2æœˆ)
                publish_time = self._extract_date_from_url(href)

                # è¿‡æ»¤è¿‡æœŸäº‹ä»¶
                if publish_time and self._is_too_old(publish_time):
                    continue

                # è§£æIPOä¿¡æ¯
                company = self._parse_ipo_company(title)
                amount = self._parse_amount(title)

                event = FundingEvent(
                    company=company,
                    title=title,
                    url=href,
                    event_type="IPO",
                    round="IPO",
                    amount=amount,
                    publish_time=publish_time,
                )
                events.append(event)

        except requests.RequestException as e:
            logger.error(f"IPOå‰çº¿è¯·æ±‚å¼‚å¸¸: {e}")

        return events

    # ================================================================
    # è§£æå·¥å…·æ–¹æ³•
    # ================================================================

    @staticmethod
    def _parse_funding_title(title: str) -> tuple[str, str, str]:
        """
        ä»èèµ„æ ‡é¢˜ä¸­æå–å…¬å¸åã€è½®æ¬¡ã€é‡‘é¢
        ä¾‹: "ç®—è‹—ç§‘æŠ€å®ŒæˆPre-Aè½®ã€Pre-A1è½®èèµ„" -> ("ç®—è‹—ç§‘æŠ€", "Pre-Aè½®", "")
        ä¾‹: "æ— ç•ŒåŠ¨åŠ›å®Œæˆè¶…2äº¿å…ƒå¤©ä½¿+è½®èèµ„" -> ("æ— ç•ŒåŠ¨åŠ›", "å¤©ä½¿+è½®", "è¶…2äº¿å…ƒ")
        """
        company = ""
        round_info = ""
        amount = ""

        # æå–å…¬å¸åï¼šé€šå¸¸åœ¨ã€Œã€æˆ–æ ‡é¢˜ä¸­"å®Œæˆ/è·"ä¹‹å‰
        # æ¨¡å¼1: ã€Œå…¬å¸åã€
        m = re.search(r'[ã€Œ"](.*?)[ã€"]', title)
        if m:
            company = m.group(1)
        else:
            # æ¨¡å¼2: é€—å·å "XXXå®Œæˆ" / "XXXè·" (å¦‚ "èšç„¦XXXï¼Œç®­å…ƒç§‘æŠ€å®ŒæˆBè½®èèµ„")
            m = re.search(r'[ï¼Œ,]\s*(.{2,15}?)(?:å®Œæˆ|è·å¾—|è·|å®£å¸ƒ|æ‹Ÿ)', title)
            if m:
                company = m.group(1).strip()
            else:
                # æ¨¡å¼3: æ ‡é¢˜å¼€å¤´åˆ° "å®Œæˆ" / "è·" / "å®£å¸ƒ"
                m = re.match(r'^(.{2,15}?)(?:å®Œæˆ|è·å¾—|è·|å®£å¸ƒ|æ‹Ÿ)', title)
                if m:
                    company = m.group(1).strip()
            # å»æ‰å¯èƒ½çš„ä¿®é¥°è¯
            if company:
                company = re.sub(r'^(æ€»é¢.*?[ï¼Œ,]|åŠå¹´.*?[ï¼Œ,])', '', company).strip()

        # æå–è½®æ¬¡
        round_patterns = [
            r'((?:Pre-?)?[A-Z]\+*è½®)',
            r'(å¤©ä½¿[\+]*è½®)',
            r'(ç§å­è½®)',
            r'(æˆ˜ç•¥èèµ„)',
            r'(è‚¡æƒèèµ„)',
            r'([A-Z]\d*\+*è½®)',
        ]
        for pattern in round_patterns:
            m = re.search(pattern, title)
            if m:
                round_info = m.group(1)
                break

        # æå–é‡‘é¢
        amount = FundingFetcher._parse_amount(title)

        return company, round_info, amount

    @staticmethod
    def _parse_amount(title: str) -> str:
        """ä»æ ‡é¢˜ä¸­æå–é‡‘é¢"""
        amount_patterns = [
            r'(è¶…[\d.]+\s*äº¿[ç¾å…ƒäººæ°‘å¸]*)',
            r'(è¿‘[\d.]+\s*äº¿[ç¾å…ƒäººæ°‘å¸]*)',
            r'(è¿‘[åç™¾åƒ]\s*äº¿[ç¾å…ƒäººæ°‘å¸]*)',
            r'([\d.]+\s*äº¿[ç¾å…ƒäººæ°‘å¸]*)',
            r'(æ•°[åƒç™¾å]ä¸‡[ç¾å…ƒäººæ°‘å¸]*)',
            r'(æ•°åƒä¸‡)',
            r'([\d.]+\s*ä¸‡[ç¾å…ƒäººæ°‘å¸]*)',
            r'(å¸‚å€¼[\d.]+\s*äº¿)',
        ]
        for pattern in amount_patterns:
            m = re.search(pattern, title)
            if m:
                return m.group(1)
        return ""

    @staticmethod
    def _parse_ipo_company(title: str) -> str:
        """ä»IPOæ ‡é¢˜ä¸­æå–å…¬å¸å"""
        # æ¨¡å¼1: ã€Œå…¬å¸åã€
        m = re.search(r'[ã€Œ"](.*?)[ã€"]', title)
        if m and len(m.group(1)) >= 2:
            return m.group(1)

        # æ¨¡å¼2: "XXXè¦IPOäº†" / "XXXæ•²é’Ÿ" / "XXXä¸Šå¸‚" / "XXXèµ´æ¸¯"
        patterns = [
            # "æ˜†ä»‘èŠ¯èµ´æ¸¯ä¸Šå¸‚" -> æ˜†ä»‘èŠ¯ï¼ˆä¼˜å…ˆåŒ¹é…æ ‡é¢˜å¼€å¤´çš„èµ´Xä¸Šå¸‚ï¼‰
            r'^(.{2,8}?)(?:èµ´æ¸¯|èµ´ç¾|èµ´çº½)',
            # "ä»Šå¤©æ™ºè°±IPOæ•²é”£" -> æ™ºè°±
            r'ä»Š[å¤©æ—¥]\s*(.{2,6}?)(?:IPO|ä¸Šå¸‚|æ•²)',
            # "æŠ¤å®¶ç§‘æŠ€è¦IPOäº†" -> æŠ¤å®¶ç§‘æŠ€
            r'[ï¼Œ,]\s*(.{2,10}?)(?:è¦IPO|IPOäº†|èµ´æ¸¯ä¸Šå¸‚|è¦ä¸Šå¸‚)',
            # "é¸£é¸£å¾ˆå¿™æ•²é’Ÿ" -> é¸£é¸£å¾ˆå¿™
            r'[ï¼Œ,]\s*(.{2,10}?)(?:æ•²[é’Ÿé”£])',
            # "ç”µç§‘è“å¤©å¸‚å€¼1000äº¿" -> ç”µç§‘è“å¤©
            r'[ï¼Œ,]\s*(.{2,8}?)(?:å¸‚å€¼)',
            # "åŒ—èŠ¯ç”Ÿå‘½æš´æ¶¨200%" -> åŒ—èŠ¯ç”Ÿå‘½
            r'[ï¼š:]\s*(.{2,8}?)(?:æš´æ¶¨|ä¸Šæ¶¨|å¤§æ¶¨|å¸‚å€¼)',
        ]
        for pattern in patterns:
            m = re.search(pattern, title)
            if m:
                name = m.group(1).strip()
                # æ¸…ç†å‰ç¼€ä¿®é¥°è¯
                name = re.sub(r'^(ä»Š[å¤©å¹´]|é¦–ä¸ª|èˆªå¤©|åŒ»ç–—|ç§‘åˆ›æ¿)', '', name).strip()
                if len(name) >= 2:
                    return name

        # å…œåº•: åŒ¹é…å¸¸è§å…¬å¸åæ¨¡å¼
        m = re.search(
            r'([\u4e00-\u9fff]{2,6}(?:ç§‘æŠ€|æ™ºèƒ½|ç”Ÿå‘½|åŒ»ç–—|èŠ¯ç‰‡|åŠå¯¼ä½“|æ–°ææ–™|èƒ½æº|èˆªå¤©|èµ„æœ¬|æ¯”è¨|é›†å›¢))',
            title,
        )
        if m:
            return m.group(1)

        # åŒ¹é…è‹±æ–‡å…¬å¸å
        m = re.search(r'([A-Z][A-Za-z]{2,15})', title)
        if m:
            return m.group(1)

        return ""

    @staticmethod
    def _extract_date_from_url(url: str) -> str:
        """ä» pedaily URL ä¸­æå–æ—¥æœŸï¼ˆæ ¼å¼: /YYYYMM/ï¼‰"""
        m = re.search(r'/(\d{4})(\d{2})/', url)
        if m:
            return f"{m.group(1)}-{m.group(2)}"
        return ""

    @staticmethod
    def _is_too_old(publish_time: str) -> bool:
        """åˆ¤æ–­äº‹ä»¶æ˜¯å¦è¿‡æœŸ"""
        now = datetime.now()
        cutoff = now - timedelta(days=FUNDING_MAX_AGE_DAYS)

        # YYYY-MM-DD HH:MM æ ¼å¼
        m = re.match(r'(\d{4})-(\d{2})-(\d{2})', publish_time)
        if m:
            try:
                event_date = datetime(int(m.group(1)), int(m.group(2)), int(m.group(3)))
                return event_date < cutoff
            except ValueError:
                pass

        # YYYY-MM æ ¼å¼ï¼ˆä»…å¹´æœˆï¼‰
        m = re.match(r'(\d{4})-(\d{2})$', publish_time)
        if m:
            try:
                year, month = int(m.group(1)), int(m.group(2))
                # å¦‚æœæ˜¯å½“æœˆæˆ–ä¸Šæœˆå°±ä¿ç•™
                if year == now.year and month >= now.month - 1:
                    return False
                if year == now.year - 1 and now.month == 1 and month == 12:
                    return False
                return True
            except ValueError:
                pass

        return False  # æ— æ³•åˆ¤æ–­æ—¶ä¿ç•™

    @staticmethod
    def _match_industry(text: str) -> Optional[str]:
        """æ ¹æ®å…³é”®è¯åŒ¹é…æ–‡æœ¬æ‰€å±è¡Œä¸š"""
        for industry, config in INDUSTRIES.items():
            for keyword in config["keywords"]:
                if keyword in text:
                    return industry

        # è¡¥å……ï¼šæŠ•èèµ„é¢†åŸŸçš„æ‰©å±•åŒ¹é…
        extended_mapping = {
            "äººå·¥æ™ºèƒ½": ["AI", "å¤§æ¨¡å‹", "æœºå™¨äºº", "æ™ºèƒ½ä½“", "ç®—åŠ›", "ç®—æ³•", "GPT", "LLM"],
            "åŠå¯¼ä½“ä¸èŠ¯ç‰‡": ["èŠ¯ç‰‡", "åŠå¯¼ä½“", "æ™¶åœ†", "EDA", "å…‰åˆ»"],
            "æ–°èƒ½æº": ["ç”µæ± ", "å…‰ä¼", "é£ç”µ", "å‚¨èƒ½", "å……ç”µ", "æ–°èƒ½æºè½¦"],
            "ç”Ÿç‰©åŒ»è¯": ["åŒ»è¯", "åŒ»ç–—", "ç”Ÿç‰©", "åŸºå› ", "è¯ç‰©", "è¯Šæ–­", "ç–«è‹—"],
            "èˆªç©ºèˆªå¤©": ["èˆªå¤©", "ç«ç®­", "å«æ˜Ÿ", "æ— äººæœº", "ä½ç©º"],
            "é«˜ç«¯è£…å¤‡åˆ¶é€ ": ["æœºå™¨äºº", "åˆ¶é€ ", "è‡ªåŠ¨åŒ–", "æ•°æ§"],
            "é‡å­ç§‘æŠ€": ["é‡å­"],
            "æ–°ææ–™": ["ææ–™", "ç¢³çº¤ç»´", "çŸ³å¢¨çƒ¯", "ç¨€åœŸ"],
            "æ•°å­—ç»æµ": ["æ•°å­—åŒ–", "æ•°æ®", "äº‘è®¡ç®—", "SaaS"],
            "ç»¿è‰²ä½ç¢³": ["ç¢³ä¸­å’Œ", "ç¯ä¿", "æ¸…æ´", "ESG"],
        }
        for industry, keywords in extended_mapping.items():
            for keyword in keywords:
                if keyword in text:
                    return industry

        return None


def fetch_funding_events() -> dict[str, list[FundingEvent]]:
    """
    ä¾¿æ·å‡½æ•°ï¼šæŠ“å–æŠ•èèµ„äº‹ä»¶å¹¶æŒ‰è¡Œä¸šåˆ†ç»„
    Returns: {è¡Œä¸šå: [FundingEvent, ...]}
    """
    fetcher = FundingFetcher()
    events = fetcher.fetch_all()

    # æŒ‰è¡Œä¸šåˆ†ç»„
    by_industry: dict[str, list[FundingEvent]] = {}
    for event in events:
        if event.industry:
            by_industry.setdefault(event.industry, []).append(event)

    return by_industry
