"""
è¡Œä¸šåŠ¨æ€æ€»ç»“æ¨¡å— - åŸºäº LLM å¯¹æŠ“å–çš„æ–°é—»ç”Ÿæˆç®€çŸ­è¡Œä¸šæ€»ç»“
Industry Summarizer - Uses LLM to generate concise industry briefings

æ€»ç»“ç»´åº¦ï¼š
- é‡ç‚¹ä¼ä¸š / æ–°å…´ä¼ä¸šåŠ¨æ€
- æŠ•èèµ„åŠ¨æ€
- äº§å“ä¸æŠ€æœ¯åŠ¨æ€
- å¸‚åœºä¸æ”¿ç­–åŠ¨æ€
- äººå‘˜ä¸ç»„ç»‡åŠ¨æ€
"""

import logging
import re
from datetime import datetime

from openai import OpenAI

from .config import LLM_API_KEY, LLM_BASE_URL, LLM_MODEL, INDUSTRIES
from .news_fetcher import NewsItem

logger = logging.getLogger(__name__)

# ============================================================
# Prompt æ¨¡æ¿
# ============================================================

SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„äº§ä¸šç ”ç©¶åˆ†æå¸ˆï¼Œæ“…é•¿ä»æ–°é—»å¿«è®¯ä¸­å¿«é€Ÿæç‚¼è¡Œä¸šåŠ¨æ€è¦ç‚¹ã€‚
ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®æä¾›çš„æ–°é—»æ ‡é¢˜å’Œæ‘˜è¦ï¼Œä¸ºæŒ‡å®šè¡Œä¸šç”Ÿæˆä¸€æ®µ**ç®€æ´ã€ä¸“ä¸šã€ä¿¡æ¯å¯†åº¦é«˜**çš„åŠ¨æ€æ€»ç»“ã€‚

å†™ä½œè¦æ±‚ï¼š
1. ç”¨ 3~6 ä¸ªè¦ç‚¹æ¦‚æ‹¬å½“å‰è¡Œä¸šæœ€å€¼å¾—å…³æ³¨çš„åŠ¨æ€ï¼Œæ¯ä¸ªè¦ç‚¹ä¸€è¡Œï¼Œä»¥ã€ŒÂ·ã€å¼€å¤´
2. è¦ç‚¹åº”æ¶µç›–ä»¥ä¸‹ç»´åº¦ï¼ˆæœ‰åˆ™å†™ï¼Œæ— åˆ™è·³è¿‡ï¼‰ï¼š
   - é‡ç‚¹ä¼ä¸š / æ–°å…´ä¼ä¸šçš„å…³é”®åŠ¨ä½œ
   - æŠ•èèµ„äº‹ä»¶ï¼ˆé‡‘é¢ã€è½®æ¬¡ã€æŠ•èµ„æ–¹ï¼‰
   - æ–°äº§å“å‘å¸ƒã€æŠ€æœ¯çªç ´
   - å¸‚åœºè¶‹åŠ¿ã€è¡Œä¸šæ•°æ®
   - æ”¿ç­–æ³•è§„ã€æ ‡å‡†åˆ¶å®š
   - é‡è¦äººäº‹å˜åŠ¨ã€ç»„ç»‡è°ƒæ•´
3. æ¯ä¸ªè¦ç‚¹æ§åˆ¶åœ¨ 30~60 å­—ï¼Œç‚¹åˆ°ä¸ºæ­¢ï¼Œä¸è¦å±•å¼€è®ºè¿°
4. æåŠå…·ä½“ä¼ä¸šåã€æ•°å­—ã€äº§å“åæ—¶è¦ä¿ç•™ï¼Œè¿™äº›æ˜¯ä¿¡æ¯å¯†åº¦çš„æ ¸å¿ƒ
5. å¦‚æœæ–°é—»å†…å®¹ä¸è¶³ä»¥æç‚¼æœ‰ä»·å€¼çš„è¦ç‚¹ï¼Œå°±æ®å®æ€»ç»“ï¼Œä¸è¦ç¼–é€ 
6. ç›´æ¥è¾“å‡ºè¦ç‚¹åˆ—è¡¨ï¼Œä¸è¦è¾“å‡ºæ ‡é¢˜ã€å‰è¨€ã€æ€»ç»“æ€§æ®µè½"""

USER_PROMPT_TEMPLATE = """ä»¥ä¸‹æ˜¯ã€{industry}ã€‘è¡Œä¸šä»Šæ—¥æŠ“å–çš„ {count} æ¡æ–°é—»ï¼š

{news_text}

è¯·æ ¹æ®ä»¥ä¸Šæ–°é—»ï¼Œè¾“å‡ºè¯¥è¡Œä¸šä»Šæ—¥åŠ¨æ€è¦ç‚¹æ€»ç»“ï¼ˆ3~6 ä¸ªè¦ç‚¹ï¼Œä»¥ã€ŒÂ·ã€å¼€å¤´ï¼‰ï¼š"""


def _build_news_text(news_items: list[NewsItem]) -> str:
    """å°†æ–°é—»åˆ—è¡¨æ ¼å¼åŒ–ä¸º LLM è¾“å…¥æ–‡æœ¬"""
    lines = []
    for i, item in enumerate(news_items, 1):
        line = f"{i}. ã€{item.title}ã€‘"
        if item.summary:
            line += f"\n   æ‘˜è¦ï¼š{item.summary[:150]}"
        if item.source:
            line += f"\n   æ¥æºï¼š{item.source}"
        lines.append(line)
    return "\n\n".join(lines)


def _summarize_with_llm(industry: str, news_items: list[NewsItem]) -> str:
    """è°ƒç”¨ LLM ç”Ÿæˆå•ä¸ªè¡Œä¸šçš„åŠ¨æ€æ€»ç»“"""
    news_text = _build_news_text(news_items)
    user_prompt = USER_PROMPT_TEMPLATE.format(
        industry=industry,
        count=len(news_items),
        news_text=news_text,
    )

    client = OpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL)

    # æ„å»ºè¯·æ±‚å‚æ•°ï¼ˆéƒ¨åˆ†æ¨¡å‹å¦‚ Kimi K2.5 ä¸æ”¯æŒè‡ªå®šä¹‰ temperatureï¼‰
    create_kwargs = {
        "model": LLM_MODEL,
        "messages": [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt},
        ],
        "max_tokens": 500,
    }

    # Kimi K2.5 æ˜¯æ€è€ƒæ¨¡å‹ï¼šä»…å…è®¸ temperature=1ï¼Œéœ€è¦æ›´å¤š token ç©ºé—´
    model_lower = LLM_MODEL.lower()
    is_thinking_model = ("kimi" in model_lower and "k2" in model_lower)

    if is_thinking_model:
        create_kwargs["temperature"] = 1.0
        create_kwargs["max_tokens"] = 2048  # æ€è€ƒè¿‡ç¨‹+å›ç­”éœ€è¦æ›´å¤šç©ºé—´
    else:
        create_kwargs["temperature"] = 0.3

    response = client.chat.completions.create(**create_kwargs)

    raw_content = response.choices[0].message.content or ""

    # Kimi K2.5 ç­‰æ€è€ƒæ¨¡å‹ä¼šåœ¨å›å¤ä¸­åŒ…å« <think>...</think> æ ‡ç­¾
    # éœ€è¦å»é™¤æ€è€ƒè¿‡ç¨‹ï¼Œåªä¿ç•™æœ€ç»ˆè¾“å‡º
    summary = _clean_thinking_tags(raw_content)
    return summary


def _clean_thinking_tags(text: str) -> str:
    """æ¸…é™¤ LLM è¿”å›å†…å®¹ä¸­çš„æ€è€ƒæ ‡ç­¾ï¼ˆ<think>...</think>ï¼‰"""
    # ç§»é™¤ <think>...</think> å—ï¼ˆå¯èƒ½è·¨å¤šè¡Œï¼‰
    cleaned = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
    # ç§»é™¤å¯èƒ½æ®‹ç•™çš„æœªé—­åˆ <think> æ ‡ç­¾
    cleaned = re.sub(r'<think>.*', '', cleaned, flags=re.DOTALL)
    # ç§»é™¤ markdown ä»£ç å—åŒ…è£¹ï¼ˆéƒ¨åˆ†æ¨¡å‹ä¼šæŠŠç»“æœæ”¾åœ¨ä»£ç å—é‡Œï¼‰
    cleaned = re.sub(r'^```[a-z]*\n?', '', cleaned.strip(), flags=re.MULTILINE)
    cleaned = re.sub(r'\n?```$', '', cleaned.strip(), flags=re.MULTILINE)
    return cleaned.strip()


def _summarize_fallback(industry: str, news_items: list[NewsItem]) -> str:
    """
    å›é€€æ–¹æ¡ˆï¼šå½“ LLM ä¸å¯ç”¨æ—¶ï¼ŒåŸºäºæ–°é—»æ ‡é¢˜ç”Ÿæˆç®€è¦æ€»ç»“
    ç›´æ¥åˆ—å‡ºæ–°é—»æ ‡é¢˜è¦ç‚¹ï¼Œä¸åšæ™ºèƒ½æ‘˜è¦
    """
    lines = []
    for item in news_items[:5]:
        title = item.title.strip()
        if len(title) > 50:
            title = title[:48] + "..."
        source = f"ï¼ˆ{item.source}ï¼‰" if item.source else ""
        lines.append(f"Â· {title}{source}")
    return "\n".join(lines)


def generate_summaries(
    news_by_industry: dict[str, list[NewsItem]],
) -> dict[str, str]:
    """
    ä¸ºæ¯ä¸ªè¡Œä¸šç”ŸæˆåŠ¨æ€æ€»ç»“

    Args:
        news_by_industry: {è¡Œä¸šå: [NewsItem, ...]}

    Returns:
        {è¡Œä¸šå: "æ€»ç»“æ–‡æœ¬"}  æ¯ä¸ªè¡Œä¸šä¸€æ®µ 3~6 è¡Œçš„è¦ç‚¹æ€»ç»“
    """
    summaries: dict[str, str] = {}
    use_llm = bool(LLM_API_KEY)

    if not use_llm:
        logger.warning(
            "LLM_API_KEY æœªé…ç½®ï¼Œå°†ä½¿ç”¨æ ‡é¢˜æ‘˜è¦æ¨¡å¼ï¼ˆå»ºè®®é…ç½® DeepSeek API ä»¥è·å¾—æ›´å¥½çš„æ€»ç»“æ•ˆæœï¼‰"
        )

    for industry, news_items in news_by_industry.items():
        if not news_items:
            continue

        emoji = INDUSTRIES.get(industry, {}).get("emoji", "ğŸ“°")
        logger.info(f"  {emoji} æ­£åœ¨æ€»ç»“ [{industry}] ...")

        try:
            if use_llm:
                summary = _summarize_with_llm(industry, news_items)
            else:
                summary = _summarize_fallback(industry, news_items)

            summaries[industry] = summary
            logger.info(f"  {emoji} [{industry}] æ€»ç»“å®Œæˆ")

        except Exception as e:
            logger.error(f"  [{industry}] æ€»ç»“ç”Ÿæˆå¤±è´¥: {e}")
            # LLM å¤±è´¥æ—¶å›é€€
            try:
                summaries[industry] = _summarize_fallback(industry, news_items)
                logger.info(f"  [{industry}] å·²ä½¿ç”¨å›é€€æ–¹æ¡ˆç”Ÿæˆæ‘˜è¦")
            except Exception as e2:
                logger.error(f"  [{industry}] å›é€€æ–¹æ¡ˆä¹Ÿå¤±è´¥: {e2}")
                summaries[industry] = "ï¼ˆæ€»ç»“ç”Ÿæˆå¤±è´¥ï¼Œè¯·æŸ¥çœ‹ä¸‹æ–¹æ–°é—»é“¾æ¥ï¼‰"

    return summaries
